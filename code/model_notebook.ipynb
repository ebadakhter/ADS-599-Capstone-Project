{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Load Packages and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebbi_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "from dateutil import parser\n",
    "from presidio_analyzer import AnalyzerEngine, EntityRecognizer, PatternRecognizer, Pattern, RecognizerResult\n",
    "from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts, NlpEngineProvider\n",
    "from presidio_analyzer.predefined_recognizers import EmailRecognizer, UrlRecognizer, PhoneRecognizer\n",
    "from presidio_analyzer.recognizer_registry import RecognizerRegistry\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avoiding usage of TEST dataset as it is the first 10 rows of TRAIN dataset\n",
    "train = pd.read_json('./../data/train.json')\n",
    "#test = pd.read_json('./../data/test.json')\n",
    "\n",
    "# Load pre-processed train and test set from EDA notebook\n",
    "preprocessed_train_df = pd.read_json('./../data/preprocess_train.json')\n",
    "#preprocessed_test_df = pd.read_json('./../data/preprocess_test.json')\n",
    "\n",
    "\n",
    "us_names = pd.read_csv('./../data/NationalNames.csv')['Name'].str.upper().unique()\n",
    "\n",
    "nltk_female = open('./../data/names/female.txt').read().split('\\n')\n",
    "nltk_male = open('./../data/names/male.txt').read().split('\\n')\n",
    "\n",
    "nltk_female = [i.upper() for i in nltk_female]\n",
    "nltk_male = [i.upper() for i in nltk_male]\n",
    "\n",
    "french_dept = pd.read_csv('./../data/departmental_names.csv')['name'].str.upper().unique()\n",
    "french_nat = pd.read_csv('./../data/national_names.csv')['name'].str.upper().unique()\n",
    "\n",
    "wikipedia = pd.read_csv('./../data/people_wiki.csv')['name'].str.upper().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(row):\n",
    "    tokens  = row['tokens']\n",
    "    start_ind = []\n",
    "    end_ind = []\n",
    "    prev_ind = 0\n",
    "    for tok in tokens:\n",
    "        start = prev_ind + row['full_text'][prev_ind:].index(tok)\n",
    "        end = start+len(tok)\n",
    "        start_ind.append(start)\n",
    "        end_ind.append(end)\n",
    "        prev_ind = end\n",
    "    return start_ind, end_ind\n",
    "\n",
    "def find_larger(arr, target):\n",
    "    left, right = 0, len(arr) - 1\n",
    "\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return left\n",
    "\n",
    "def count_trailing_whitespaces(word):\n",
    "    return len(word) - len(word.rstrip())\n",
    "\n",
    "def is_valid_date(text):\n",
    "    try:\n",
    "        parsed_date = parser.parse(text)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def pii_fbeta_score(pred_df, gt_df,beta=5):\n",
    "\n",
    "    df = pred_df.merge(gt_df,how='outer',on=['document',\"token\"],suffixes=('_pred','_gt'))\n",
    "\n",
    "    df['cm'] = \"\"\n",
    "\n",
    "    df.loc[df.label_gt.isna(),'cm'] = \"FP\"\n",
    "    df.loc[df.label_pred.isna(),'cm'] = \"FN\"\n",
    "    df.loc[(df.label_gt.notna()) & (df.label_gt!=df.label_pred),'cm'] = \"FN\"\n",
    "\n",
    "    df.loc[(df.label_pred.notna()) & (df.label_gt.notna()) & (df.label_gt==df.label_pred),'cm'] = \"TP\"\n",
    "    \n",
    "    FP = (df['cm']==\"FP\").sum()\n",
    "    FN = (df['cm']==\"FN\").sum()\n",
    "    TP = (df['cm']==\"TP\").sum()\n",
    "    \n",
    "    precision = TP/(TP + FP)\n",
    "    recall = TP/(TP + FN)\n",
    "    f1 = precision * recall / (precision + recall)\n",
    "    \n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"F1-Score \" + str(f1))\n",
    "\n",
    "    s_micro = (1+(beta**2))*TP/(((1+(beta**2))*TP) + ((beta**2)*FN) + FP)\n",
    "\n",
    "    return s_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(preprocessed_train_df)\n",
    "y = x['labels']\n",
    "x = x.drop(columns='labels')\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_X: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5105 entries, 276 to 2732\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   document             5105 non-null   int64 \n",
      " 1   full_text            5105 non-null   object\n",
      " 2   tokens               5105 non-null   object\n",
      " 3   trailing_whitespace  5105 non-null   object\n",
      " 4   tokens_processed     5105 non-null   object\n",
      " 5   tokens_count         5105 non-null   int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 279.2+ KB\n",
      "Val_X: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1702 entries, 3561 to 2448\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   document             1702 non-null   int64 \n",
      " 1   full_text            1702 non-null   object\n",
      " 2   tokens               1702 non-null   object\n",
      " 3   trailing_whitespace  1702 non-null   object\n",
      " 4   tokens_processed     1702 non-null   object\n",
      " 5   tokens_count         1702 non-null   int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 93.1+ KB\n",
      "Train_Y: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 5105 entries, 276 to 2732\n",
      "Series name: labels\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "5105 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 79.8+ KB\n",
      "Val_Y: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 1702 entries, 3561 to 2448\n",
      "Series name: labels\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "1702 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 26.6+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"Train_X: \")\n",
    "train_x.info()\n",
    "print(\"Val_X: \")\n",
    "val_x.info()\n",
    "print(\"Train_Y: \")\n",
    "train_y.info()\n",
    "print(\"Val_Y: \")\n",
    "val_y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOW_LIST = []\n",
    "DENY_LIST_EMAIL = []\n",
    "DENY_LIST_ADDRESS = []\n",
    "DENY_LIST_URL = []\n",
    "DENY_LIST_NAME = []\n",
    "DENY_LIST_PHONE = []\n",
    "DENY_LIST_ID = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = list(stopwords.words())\n",
    "words = Counter()\n",
    "for doc in preprocessed_train_df.tokens_processed:\n",
    "    words.update(doc)\n",
    "#Disabled for not using the lesser test records\n",
    "#for doc in preprocessed_test_df.tokens_processed:\n",
    "#    words.update(doc)\n",
    "all_stopwords  += [str(w).lower() for w, i in words.items() if i > 55]\n",
    "all_stopwords = list(sorted(set(all_stopwords)))\n",
    "del words\n",
    "\n",
    "ALLOW_LIST.extend(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHONE_LIST = ['phone', 'number', 'telephone', 'cell', 'cellphone',\n",
    "              'mobile', 'call', 'ph', 'tel', 'mobile', 'Email']\n",
    "URL_LIST = [\"wikipedia\", \"coursera\", \".pdf\", \".PDF\", \"article\",\n",
    "             \".png\",\".gov\", \".work\", \".ai\", \".firm\", \".arts\",\".store\",\n",
    "              \".rec\", \".biz\", \".travel\",'.ru', 'designabetterbusiness',\n",
    "               '.tools', 'designorate','designresearchtechniques', \n",
    "               'ec', '.europa', 'forbes', 'google','ideas', 'trello', '.edu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Training\"\n",
    "#Iterate through all of the data, identify tokens that are labels, add to allow/deny list\n",
    "\n",
    "DENY_LIST_NAME.extend(us_names)\n",
    "DENY_LIST_NAME.extend(nltk_female)\n",
    "DENY_LIST_NAME.extend(nltk_male)\n",
    "DENY_LIST_NAME.extend(french_dept)\n",
    "DENY_LIST_NAME.extend(french_nat)\n",
    "\n",
    "tokens = train_x['tokens'].apply(pd.Series).stack().reset_index(drop=True).tolist()\n",
    "labels = train_y.apply(pd.Series).stack().reset_index(drop=True).tolist()\n",
    "\n",
    "for i in set(labels):\n",
    "    indices = [j for j in range(len(labels)) if labels[j] == i]\n",
    "    if i == 'O':\n",
    "        ALLOW_LIST.extend([tokens[i] for i in indices])\n",
    "    if i == 'B-EMAIL':\n",
    "        DENY_LIST_EMAIL.extend([tokens[i] for i in indices])\n",
    "    elif i in ['B-STREET_ADDRESS', 'I-STREET_ADDRESS']:\n",
    "        DENY_LIST_ADDRESS.extend([tokens[i] for i in indices])\n",
    "    elif i in ['B-URL_PERSONAL', 'I-URL_PERSONAL']:\n",
    "        DENY_LIST_URL.extend([tokens[i] for i in indices])\n",
    "    elif i in ['B-NAME_STUDENT', 'I-NAME_STUDENT']:\n",
    "    #elif i in ['I-NAME_STUDENT']:\n",
    "        DENY_LIST_NAME.extend([tokens[i] for i in indices])\n",
    "    elif i in ['B-PHONE_NUM', 'I-PHONE_NUM']:\n",
    "        DENY_LIST_PHONE.extend([tokens[i] for i in indices])\n",
    "    elif i in ['B-ID_NUM', 'I-ID_NUM']:\n",
    "        DENY_LIST_ID.extend([tokens[i] for i in indices])\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_regex = r'([A-Za-z]{2}[.?]:)?\\d{12,12}'\n",
    "id_pattern = Pattern(name=\"id\", regex=id_regex, score = 0.5)\n",
    "id_recognizer = PatternRecognizer(supported_entity=\"ID_CUSTOM\", patterns = [id_pattern])\n",
    "\n",
    "address_regex = r'\\b\\d+\\s+\\w+(\\s+\\w+)*\\s+((st(\\.)?)|(ave(\\.)?)|(cir(\\.)?)|(rd(\\.)?)|(blvd(\\.)?)|(ln(\\.)?)|(ct(\\.)?)|(dr(\\.)?))\\b'\n",
    "address_pattern = Pattern(name=\"address\", regex=address_regex, score=0.5)\n",
    "address_recognizer = PatternRecognizer(supported_entity=\"ADDRESS_CUSTOM\", patterns = [address_pattern], context=[\"st\", \"Apt\"])\n",
    "\n",
    "email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "email_pattern = Pattern(name=\"email address\", regex=email_regex, score=0.5)\n",
    "email_recognizer = PatternRecognizer(supported_entity=\"EMAIL_CUSTOM\", patterns = [email_pattern])\n",
    "\n",
    "url_regex = r'((https?)|(http?)|(ftp?))://\\S+|www\\.\\S+'\n",
    "url_pattern = Pattern(name=\"url\", regex=url_regex, score=0.5)\n",
    "url_recognizer = PatternRecognizer(supported_entity=\"URL_CUSTOM\", patterns = [url_pattern])\n",
    "\n",
    "phone_regex = r'^[\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}$'\n",
    "phone_pattern = Pattern(name='phone', regex=phone_regex, score=0.5)\n",
    "phone_recognizer = PatternRecognizer(supported_entity='PHONE_CUSTOM', patterns=[phone_pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Microsoft Number Recognizer to consider lettered-numbers as well\n",
    "class NumbersRecognizer(EntityRecognizer):\n",
    "\n",
    "    expected_confidence_level = 0.7  # expected confidence level for this recognizer\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \"\"\"No loading is required.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def analyze(self, text: str, entities: list[str], nlp_artifacts: NlpArtifacts) -> list[RecognizerResult]:\n",
    "        \"\"\"\n",
    "        Analyzes test to find tokens which represent numbers (either 123 or One Two Three).\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # iterate over the spaCy tokens, and call `token.like_num`\n",
    "        for token in nlp_artifacts.tokens:\n",
    "            if token.like_num:\n",
    "                result = RecognizerResult(\n",
    "                    entity_type=\"NUMBER\",\n",
    "                    start=token.idx,\n",
    "                    end=token.idx + len(token),\n",
    "                    score=self.expected_confidence_level,\n",
    "                )\n",
    "                results.append(result)\n",
    "        return results\n",
    "\n",
    "new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n",
    "}\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine = provider.create_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = RecognizerRegistry()\n",
    "dictionary.load_predefined_recognizers()\n",
    "dictionary.add_recognizer(address_recognizer)\n",
    "dictionary.add_recognizer(email_recognizer)\n",
    "dictionary.add_recognizer(url_recognizer)\n",
    "dictionary.add_recognizer(phone_recognizer)\n",
    "dictionary.add_recognizer(id_recognizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = AnalyzerEngine(supported_languages=['en'],\n",
    "                          registry=dictionary,\n",
    "                          nlp_engine=nlp_engine,\n",
    "                          context_aware_enhancer=LemmaContextAwareEnhancer(\n",
    "                              context_similarity_factor=0.6,\n",
    "                              min_score_with_context_similarity=0.4\n",
    "                          ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainnig and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "#Testing on the split documents, as they are higher in count than the original test.json file\n",
    "\n",
    "#test = preprocessed_test_df\n",
    "test = val_x\n",
    "\n",
    "temp = test.apply(lambda x: find_index(x), axis=1)\n",
    "test['start'] = temp.apply(lambda x: x[0])\n",
    "test['end'] = temp.apply(lambda x: x[1])\n",
    "#pii_labels = [\n",
    "#    'B-NAME_STUDENT', 'I-NAME_STUDENT',\n",
    "#    'B-URL_PERSONAL', 'I-URL_PERSONAL',\n",
    "#    'B-ID_NUM', 'I-ID_NUM',\n",
    "#    'B-EMAIL', 'I-EMAIL',\n",
    "#    'B-STREET_ADDRESS', 'I-STREET_ADDRESS',\n",
    "#    'B-PHONE_NUM', 'I-PHONE_NUM',\n",
    "#    'B-USERNAME', 'I-USERNAME'\n",
    "#]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1702it [03:37,  7.82it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(tqdm(test.iterrows())):\n",
    "    results = analyzer.analyze(text=d[1]['full_text'],\n",
    "                               entities= [\"PHONE_CUSTOM\", \"PERSON\", \"URL_CUSTOM\", \"EMAIL_ADDRESS\",\n",
    "                                          \"EMAIL_CUSTOM\", \"ADDRESS_CUSTOM\", \"US_SSN\", \"US_ITIN\",\n",
    "                                          \"US_PASSPORT\", \"US_BANK_NUMBER\", \"USERNAME\", \"ID_CUSTOM\"],                                         \n",
    "                               allow_list=ALLOW_LIST,\n",
    "                               language='en', \n",
    "                               score_threshold=0.005)\n",
    "    pre_preds = []\n",
    "    for r in results:\n",
    "        s = find_larger(d[1]['start'], r.start)\n",
    "        end = r.end\n",
    "        word = d[1]['full_text'][r.start:r.end]\n",
    "        end = end - count_trailing_whitespaces(word)\n",
    "        temp_preds = [s]\n",
    "        try:\n",
    "            while d[1]['end'][s+1] <= end:\n",
    "                temp_preds.append(s+1)\n",
    "                s +=1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        tmp = False\n",
    "        \n",
    "        #'B-PHONE_NUM', 'I-PHONE_NUM',\n",
    "        if r.entity_type in ('PHONE_CUSTOM', 'B-PHONE_NUM', 'I-PHONE_NUM'):\n",
    "            if is_valid_date(word):\n",
    "                continue\n",
    "            for w in PHONE_LIST:\n",
    "                if w in d[1]['full_text'][max(r.start-50, 0):min(r.end+50, len(d[1]['full_text']))]:\n",
    "                    tmp = False\n",
    "                    break\n",
    "                else:\n",
    "                    tmp = True \n",
    "            label =  'PHONE_NUM'\n",
    "        #'B-NAME_STUDENT', 'I-NAME_STUDENT',\n",
    "        if r.entity_type in ('PERSON', 'B-NAME_STUDENT', 'I-NAME_STUDENT'):\n",
    "            if str(i).upper() in wikipedia:\n",
    "                tmp = True\n",
    "                break\n",
    "            label =  'NAME_STUDENT'\n",
    "        #'B-URL_PERSONAL', 'I-URL_PERSONAL',\n",
    "        if r.entity_type in ('URL_CUSTOM', 'B-URL_PERSONAL', 'I-URL_PERSONAL'):\n",
    "            for w in URL_LIST:\n",
    "                if w in word:\n",
    "                    tmp = True\n",
    "                    break\n",
    "            label = 'URL_PERSONAL'\n",
    "        #'B-EMAIL', 'I-EMAIL', \n",
    "        if r.entity_type in ('B-EMAIL', 'I-EMAIL','EMAIL_ADDRESS', 'EMAIL_CUSTOM'):\n",
    "            label = \"EMAIL\"\n",
    "        #'B-STREET_ADDRESS', 'I-STREET_ADDRESS',\n",
    "        if r.entity_type in ('B-STREET_ADDRESS', 'ADDRESS_CUSTOM', 'I-STREET_ADDRESS'):\n",
    "            label = 'STREET_ADDRESS'\n",
    "        #'B-ID_NUM', 'I-ID_NUM',\n",
    "        if r.entity_type in ['B-ID_NUM', 'I-ID_NUM', 'US_SSN', 'US_ITIN', 'US_PASSPORT', 'US_BANK_NUMBER', 'ID_CUSTOM']:\n",
    "            label = 'ID_NUM'\n",
    "        #'B-USERNAME', 'I-USERNAME'\n",
    "        if r.entity_type in ['B-USERNAME', 'I-USERNAME', 'USERNAME']:\n",
    "            label =  'USERNAME'\n",
    "        if tmp:\n",
    "            continue\n",
    "        for p in temp_preds:\n",
    "            if len(pre_preds) > 0:\n",
    "                if pre_preds[-1]['rlabel'] == r.entity_type and ((p - pre_preds[-1]['token'])==1):\n",
    "                    label_f = \"I-\"+label\n",
    "                else:\n",
    "                    label_f = \"B-\"+label\n",
    "            else:\n",
    "                label_f = \"B-\"+label\n",
    "            pre_preds.append(({\n",
    "                    \"document\":d[1]['document'],\n",
    "                    \"token\":p,\n",
    "                    \"label\":label_f,\n",
    "                    \"rlabel\":r.entity_type\n",
    "                }))\n",
    "    preds.extend(pre_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame(preds).iloc[:,:-1].reset_index()\n",
    "final_results.columns = ['row_id','document', 'token', 'label']\n",
    "\n",
    "temp = val_x[['document']].join(val_y)\n",
    "\n",
    "dictionary = temp['labels'].apply(lambda x: {'indx': list(range(len(x))), 'vals': x})\n",
    "indices = dictionary.apply(lambda x: x['indx']).explode()\n",
    "values = dictionary.apply(lambda x: x['vals']).explode()\n",
    "\n",
    "ground_truth = pd.concat([indices, values], axis=1).reset_index()\n",
    "ground_truth['document'] = ground_truth['index'].apply(lambda x: temp['document'][x])\n",
    "ground_truth = ground_truth.drop(columns='index')\n",
    "ground_truth.columns = ['token', 'label', 'document']\n",
    "ground_truth = ground_truth[ground_truth['label'] != 'O']\n",
    "ground_truth = ground_truth.reset_index(names=['row_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.27905004240882103\n",
      "Recall: 0.8255959849435383\n",
      "F1-Score 0.20855784469096672\n",
      "0.7677601759188619\n"
     ]
    }
   ],
   "source": [
    "print(pii_fbeta_score(final_results, ground_truth, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
